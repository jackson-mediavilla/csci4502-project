{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods To Import Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function will take as an argument a file containing\n",
    "the field index and description for every field in the data frames.\n",
    "I will use this to create a template dataframe and to name the columns\n",
    "because currently it is just using the first value as the column name\n",
    "and it makes literally no sense at all.\n",
    "'''\n",
    "def shape_dataframe():\n",
    "    #this file contains all the field descriptions\n",
    "    field_descriptor_file_path = \"csvFieldDescriptions.txt\"\n",
    "    \n",
    "    #open the file and read it, adding each description to a list\n",
    "    fields = open(field_descriptor_file_path, 'r')\n",
    "    fieldInfo = []\n",
    "    for field in fields:\n",
    "        fieldInfo.append(field[2:].strip())\n",
    "    return fieldInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as an argument the number of subframes to include\n",
    "where each 'subframe' is a pandas data frame containing play by play\n",
    "data for a team for a year. It will then return those subframes\n",
    "concatenated together into one main data frame. Subframes is preset to 50\n",
    "'''\n",
    "def create_dataframe(subframes=50):\n",
    "    csv_paths = os.listdir('./csvFiles')#Get the paths of all of the CSV files\n",
    "    column_info = shape_dataframe()#get the information on each field\n",
    "    data = pd.DataFrame(columns=column_info)#create an empty data frame\n",
    "    individual_data = []#create a list to hold the smaller frames\n",
    "    del individual_data[:]#clear the list just in case I've already been using it\n",
    "\n",
    "    #make all the dataframes\n",
    "    df = pd.DataFrame(columns=column_info)\n",
    "    for index, path in enumerate(csv_paths):#iterate over the list of paths\n",
    "        #names=column_info is what names the columns\n",
    "        df = pd.read_csv(str('./csvFiles/'+path), names=column_info)#read a file into a csv\n",
    "        individual_data.append(df)#add it to the list\n",
    "        if(index==50):#just so my computer doesn't die a slow death\n",
    "            break\n",
    "    data = pd.concat(individual_data) #combining sub frames\n",
    "    del individual_data[:] #dont wanna waste space\n",
    "    return data#, individual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method To Import Game Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in all the game logs and appending the dataframe accordingly\n",
    "def get_gl_data():\n",
    "    GLData = pd.DataFrame()\n",
    "    gls = [] #to hold smaller frames\n",
    "    del gls[:] #clear it to be sure\n",
    "    glogpath = '../data/GameLogs/' #path to the game logs\n",
    "    headPath = glogpath+'game_log_header.csv' #get the column info\n",
    "    header = pd.read_csv(headPath) #import the column info\n",
    "    colInfo = header.columns #store it for later use\n",
    "    start_year = 1950 #define starting year. gonna use this in path\n",
    "    end_year = 17 #also for path\n",
    "    for logFolder in os.listdir(glogpath):\n",
    "        try:#catching non int cases\n",
    "            y1 = int(logFolder[2:4]) ##first two of start yyyy\n",
    "            y2 = int(logFolder[2:6]) ##full start year\n",
    "            y3 = int(str(y1)+logFolder[-2:]) #full end year\n",
    "        except:\n",
    "            continue\n",
    "        #now open the folder if the start year is between the dates in the name\n",
    "        if(start_year<y3):\n",
    "            newPath = glogpath+logFolder\n",
    "            files = os.listdir(newPath)#get all the logs in the folder\n",
    "            for file in files:#now check each file to make sure it's the righ year\n",
    "                year = int(file[2:6])#get year of file\n",
    "                if(year>=start_year):#if it's within the range we want\n",
    "                    full_path = glogpath+logFolder+'/'+file #make full path\n",
    "                    gls.append(pd.read_csv(full_path, names=colInfo))#append the frame\n",
    "    GLData = pd.concat(gls) #combine the subframes into this one\n",
    "    del gls[:] #not wasting space\n",
    "    return GLData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EVdata = create_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLData = get_gl_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should probably turn these into functions so I can also preprocess the main dataset. or any of the data that I want to pass to it. Shouldn't be to hard to parameterize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to see what fraction of the original size the pruned data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this will just return how much memory pre, post and the percentage\n",
    "def get_reduction(pre, post):\n",
    "    a = np.sum(pre.memory_usage())\n",
    "    b = np.sum(post.memory_usage())\n",
    "    return a, b, b/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to drop a lot of columns\n",
    "\n",
    "#indexes for columns to drop. Honestly i just looked at the field descriptions\n",
    "#and dropped mostly things like who was playing each position, where the ball \n",
    "#was hit, the names of people who contributed to the play, etc\n",
    "ix = [12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 32, 46, 47,\n",
    "      49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
    "      67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86,\n",
    "      87, 88, 89, 90, 91, 92, 93, 94, 95, 96\n",
    "     ]\n",
    "\n",
    "EVdatap = EVdata #copy data for pruned data\n",
    "original_data_size = np.sum(EVdata.memory_usage(deep=True)) #how big is the data\n",
    "\n",
    "#Prune Columns\n",
    "EVdatap = EVdatap.drop(EVdatap.columns[ix], axis=1) #prune columns\n",
    "\n",
    "#Here, I am defining columns where if the value is null, the row should\n",
    "#be dropped. These are mainly things like sacrifice hits or bunts\n",
    "ixr = [21, 22, 29]\n",
    "\n",
    "#drop rows with missing values that may skew batting averages\n",
    "#Namely sacrifice hits and bunts\n",
    "EVdatap = EVdatap.dropna(axis=1, subset=ixr)\n",
    "\n",
    "#drop ixr columns too now that they've been used\n",
    "EVdatap = EVdatap.drop(EVdatap.columns[ixr], axis=1) #drop NaN\n",
    "pruned_data_size = np.sum(EVdatap.memory_usage(deep=True)) #store size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding in a bit more data on the team, date, game of day, and also field\n",
    "EVdatap['team'] = EVdatap['game id'].astype(str).str[:3]#storing the team\n",
    "EVdatap['date'] = EVdatap['game id'].astype(str).str[3:-1]#date\n",
    "EVdatap['game_of_day'] = EVdatap['game id'].astype(str).str[-1:]#which game of the day it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Categorical Data to Ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "redata = EVdatap #reduced event data\n",
    "\n",
    "#I noticed all the features I wanted to convert were all binary data for\n",
    "#flags and which hand people use. So gonna iterate through the features\n",
    "#and convert features with the word flag or hand\n",
    "for column in redata.columns:#go through the cols\n",
    "    #if the feature name has hand or flag\n",
    "    if(('hand' in column.lower()) or ('flag' in column.lower())):\n",
    "        redata[column] = redata[column].astype('category')#set to cat\n",
    "\n",
    "#convert them to ints            \n",
    "cat_columns = redata.select_dtypes(['category']).columns #get cat cols\n",
    "redata[cat_columns] = redata[cat_columns].apply(lambda x: x.cat.codes) #cast to int\n",
    "ints = redata.select_dtypes(include=['int8', 'int64', 'int']) #get any int cols\n",
    "ints = ints.columns #get feature names of ints\n",
    "#downcast all to smallest acceptable int size\n",
    "redata[ints] = redata[ints].apply(lambda x: pd.to_numeric(x, downcast='unsigned'))\n",
    "retyped_data_size = np.sum(redata.memory_usage(deep=True)) #store size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101385303 343176981 164185596\n",
      "1.0 0.31158667186246264 0.14907189659493758\n"
     ]
    }
   ],
   "source": [
    "print(original_data_size, pruned_data_size, retyped_data_size)\n",
    "print(original_data_size/original_data_size, pruned_data_size/original_data_size, retyped_data_size/original_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see on the top row, from left to right, the original data size, the size after pruning, and the size after downcasting the ints. On the bottom, from left to right, we see the fraction of the original size for the original size, the pruned, and the downcasted data. We see that we have been able to achieve a reduction in size of about 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data9 = EVdata.loc[data.inning==9]\n",
    "data92 = data9[abs(data9['vis score']-data9['home score'])<2]#9th inning with two outs\n",
    "x = data9['outs']\n",
    "y = data9['RBI on play']\n",
    "correlation = np.corrcoef(x, y)\n",
    "EVdata[\"outOrHit\"] = EVdata.apply(lambda row: 1 if \n",
    "                                (row[\"event type\"]==2 or \n",
    "                                row[\"event type\"]==3) else\n",
    "                                2 if \n",
    "                                row[\"event type\"]==20 else\n",
    "                                3 if\n",
    "                                row[\"event type\"]==21 else\n",
    "                                4 if\n",
    "                                row[\"event type\"]==22 else\n",
    "                                5 if\n",
    "                                row[\"event type\"]==23\n",
    "                                else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.hist(EVdata.loc[data.outs==2].outOrHit.dropna(), facecolor='green')\n",
    "ax.set_xlabel(\"Event Value\\nFigure 2\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xticks(np.arange(6))\n",
    "ax.set_title(\"Frequency of Event Values with 2 Outs\")\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure and axis\n",
    "fig, ax = plt.subplots(figsize=(4,12), nrows=3, ncols=1)\n",
    "for i in data.loc[data.inning==9].outs.unique():\n",
    "    EVdata.loc[(data.inning==9) & (data.outs==i)].outOrHit.hist(ax=ax[i])\n",
    "    ax[i].set_xlabel(\"Event Value\\nFigure 1\")\n",
    "    ax[i].set_ylabel(\"Frequency\")\n",
    "    ax[i].set_xticks(np.arange(6))\n",
    "    ax[i].set_title(\"Frequency of Event Values with {0} Outs\".format(i))\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "plt.suptitle(\"Frequency of Event Values with 0, 1, and 2 Outs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying To Make Some Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
