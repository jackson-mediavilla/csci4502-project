{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import os\n",
    "import re\n",
    "\n",
    "from scipy import stats\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods To Import Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function will take as an argument a file containing\n",
    "the field index and description for every field in the data frames.\n",
    "I will use this to create a template dataframe and to name the columns\n",
    "because currently it is just using the first value as the column name\n",
    "and it makes literally no sense at all.\n",
    "'''\n",
    "def shape_ev_data():\n",
    "    #this file contains all the field descriptions\n",
    "    field_descriptor_file_path = \"csvFieldDescriptions.txt\"\n",
    "    \n",
    "    #open the file and read it, adding each description to a list\n",
    "    fields = open(field_descriptor_file_path, 'r')\n",
    "    fieldInfo = []\n",
    "    for field in fields:\n",
    "        fieldInfo.append(field[2:].strip())\n",
    "    fields.close() #close file\n",
    "    return fieldInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as an argument the number of subframes to include\n",
    "where each 'subframe' is a pandas data frame containing play by play\n",
    "data for a team for a year. It will then return those subframes\n",
    "concatenated together into one main data frame. Subframes is preset to 50\n",
    "'''\n",
    "def get_event_data(subframes=50):\n",
    "    csvFiles = '../data/event_csv_files/'\n",
    "    csv_paths = os.listdir(csvFiles)#Get the paths of all of the CSV files\n",
    "    column_info = shape_ev_data()#get the information on each field\n",
    "    data = pd.DataFrame(columns=column_info)#create an empty data frame\n",
    "    individual_data = []#create a list to hold the smaller frames\n",
    "    del individual_data[:]#clear the list just in case I've already been using it\n",
    "\n",
    "    #make all the dataframes\n",
    "    df = pd.DataFrame(columns=column_info) #define a temp frame\n",
    "    for index, path in enumerate(csv_paths):#iterate over the list of paths\n",
    "        #names=column_info is what names the columns\n",
    "        df = pd.read_csv(str(csvFiles+path), names=column_info)#read a file into a csv\n",
    "        individual_data.append(df)#add it to the list\n",
    "        \n",
    "        #this line here is what limits how much data you pull.\n",
    "        #if you eneter -1 for subframes it'll skip this check and\n",
    "        #generate all of the data\n",
    "        if(subframes != -1):#if the passed parameter is -1, generate ALL data\n",
    "            if(index==subframes):#stop when desired subframe # is reached\n",
    "                break\n",
    "    \n",
    "    data = pd.concat(individual_data) #combining sub frames\n",
    "    del individual_data[:] #dont wanna waste space\n",
    "    return data#, individual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method To Import Game Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in all the game logs and appending the dataframe accordingly\n",
    "def get_gl_data():\n",
    "    GLData = pd.DataFrame()\n",
    "    gls = [] #to hold smaller frames\n",
    "    del gls[:] #clear it to be sure\n",
    "    glogpath = '../data/GameLogs/' #path to the game logs\n",
    "    headPath = glogpath+'game_log_header.csv' #get the column info\n",
    "    header = pd.read_csv(headPath) #import the column info\n",
    "    colInfo = header.columns #store it for later use\n",
    "    start_year = 1950 #define starting year. gonna use this in path\n",
    "    end_year = 17 #also for path\n",
    "    for logFolder in os.listdir(glogpath):\n",
    "        try:#catching non int cases\n",
    "            y1 = int(logFolder[2:4]) ##first two of start yyyy\n",
    "            y2 = int(logFolder[2:6]) ##full start year\n",
    "            y3 = int(str(y1)+logFolder[-2:]) #full end year\n",
    "        except:\n",
    "            continue\n",
    "        #now open the folder if the start year is between the dates in the name\n",
    "        if(start_year<y3):\n",
    "            newPath = glogpath+logFolder\n",
    "            files = os.listdir(newPath)#get all the logs in the folder\n",
    "            for file in files:#now check each file to make sure it's the righ year\n",
    "                year = int(file[2:6])#get year of file\n",
    "                if(year>=start_year):#if it's within the range we want\n",
    "                    full_path = glogpath+logFolder+'/'+file #make full path\n",
    "                    gls.append(pd.read_csv(full_path, names=colInfo))#append the frame\n",
    "    GLData = pd.concat(gls) #combine the subframes into this one\n",
    "    del gls[:] #not wasting space\n",
    "    return GLData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method To Import Park Information Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this method will return an unedited dataframe of the park information\n",
    "def get_park_data():\n",
    "    parks = pd.DataFrame() #create empty frame\n",
    "    path = '../data/ParkInfo/ParkCodes.TXT' #path to park data\n",
    "\n",
    "    cols = ['PARKID','CITY','STATE'] #cols to import\n",
    "    parks = pd.read_csv(path, usecols=cols) #import it as csv\n",
    "    parks.columns = ['parkid','city','state'] #name cols\n",
    "    parks['location'] = parks.city + ', ' + parks.state #easier to cross ref\n",
    "    parks = parks.drop(['city','state'],axis=1)\n",
    "    \n",
    "    #parks['lat'] = 0\n",
    "    #parks['lng'] = 0\n",
    "    \n",
    "    return parks #return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method To Import City Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this method imports data on cities in the us\n",
    "def get_ct_data():\n",
    "    cities = pd.DataFrame() #create empty frame\n",
    "    path = '../data/ParkInfo/uscities.csv' #path\n",
    "    \n",
    "    #which columns to import\n",
    "    cols = ['city','state_id','lat','lng',]\n",
    "    \n",
    "    #get park info but only read the cols i want\n",
    "    cities = pd.read_csv(path, usecols=cols)\n",
    "    \n",
    "    cities.columns = ['city','state','lat','lng'] #rename cols\n",
    "    cities['location'] = cities.city + ', ' + cities.state #for cross ref\n",
    "    cities = cities.drop(['city','state'], axis=1) #drop old cols\n",
    "    \n",
    "    cities = cities.drop_duplicates(subset='location') #only one station per city\n",
    "    \n",
    "    return cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method To Import Station Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the station data\n",
    "def get_st_data():\n",
    "    cols = ['id','lat','lng','elevation','state']\n",
    "    stations = pd.DataFrame(columns=cols) #stations\n",
    "    path = '../data/Weather/station_list.txt' #path to file\n",
    "    formatted_path = '../data/Weather/formatted_station_list.txt'\n",
    "    stations = open(path, 'r') #open weather stations\n",
    "    f_stations = open(formatted_path,'w') #file to write to\n",
    "    \n",
    "    #reformat the weather station file so it's not this ridiculous format\n",
    "    for line in stations: #for each line\n",
    "        new = line[:41] #remove the end because that's all irrelevant\n",
    "        new = re.sub('\\s\\s$', 'zz', new)#fake state col if it doesn't exist\n",
    "        new = ','.join(new.split())#remove spaces and replace with commas\n",
    "        new = new+'\\n' #add a newline at the end\n",
    "        f_stations.write(new) #write the formatted line\n",
    "    f_stations.close() #close the file at the end\n",
    "    \n",
    "    #now set stations frame\n",
    "    stations = pd.read_csv(formatted_path, names=cols)\n",
    "    \n",
    "    stations = stations.replace('zz',np.nan)#replace the zz\n",
    "    stations = stations.dropna(subset=['state'],axis=0)#drop ones with state (not in us)\n",
    "    \n",
    "    return stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method To Import bgame Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get cols of feature names for bgame files\n",
    "def shape_gl_data():\n",
    "    fn = open('bgameFeatureNames.TXT', 'r') #open file\n",
    "    fnames = [] #to store names\n",
    "    for feature in fn: #go over lines\n",
    "        f = re.sub('[\\(\\[].*?[\\)\\]]', '', feature[5:]).strip() #strip it\n",
    "        fnames.append(f) #add to list\n",
    "    fn.close()#close file\n",
    "    return fnames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gets game log data from bgame\n",
    "def get_gl2_data():\n",
    "    csvFiles = '../data/game_csv_files/'\n",
    "    csv_paths = os.listdir(csvFiles)#Get the paths of all of the CSV files\n",
    "    column_info = shape_gl_data()#get the information on each field\n",
    "    data = pd.DataFrame(columns=column_info)#create an empty data frame\n",
    "    individual_data = []#create a list to hold the smaller frames\n",
    "    del individual_data[:]#clear the list just in case I've already been using it\n",
    "\n",
    "    #make all the dataframes\n",
    "    df = pd.DataFrame(columns=column_info) #define a temp frame\n",
    "    for index, path in enumerate(csv_paths):#iterate over the list of paths\n",
    "        #names=column_info is what names the columns\n",
    "        df = pd.read_csv(str(csvFiles+path), names=column_info)#read a file into a csv\n",
    "        individual_data.append(df)#add it to the list\n",
    "    \n",
    "    data = pd.concat(individual_data) #combining sub frames\n",
    "    del individual_data[:] #dont wanna waste space\n",
    "    return data#, individual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should probably turn these into functions so I can also preprocess the main dataset. or any of the data that I want to pass to it. Shouldn't be to hard to parameterize. I guess it's worth noting that all of these functions so far have been written with the event data in mind. They almost certainly will not work on the gamelog data. I'm gonna rename the functions to indicate what data they should be used on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to see what fraction of the original size the pruned data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this will just return how much memory pre, post and the percentage\n",
    "def get_reduction(pre, post):\n",
    "    a = np.sum(pre.memory_usage())\n",
    "    b = np.sum(post.memory_usage())\n",
    "    return a, b, b/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to drop a lot of columns\n",
    "def prune_ev_data(original_data, verbose=False):\n",
    "    #indexes for columns to drop. Honestly i just looked at the field descriptions\n",
    "    #and dropped mostly things like who was playing each position, where the ball \n",
    "    #was hit, the names of people who contributed to the play, etc\n",
    "    ix = [12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 32, 46, 47,\n",
    "          49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
    "          67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86,\n",
    "          87, 88, 89, 90, 91, 92, 93, 94, 95, 96\n",
    "         ]\n",
    "    #gonna turn those indexes into names just to ensure i drop the right\n",
    "    #columns\n",
    "    ixn = original_data.columns[ix]\n",
    "    \n",
    "    #verbose option\n",
    "    if(verbose):\n",
    "        print(\"Dropping Columns:\\n\")\n",
    "        for col in ixn:\n",
    "            print(col)\n",
    "    \n",
    "    pruned_data = original_data #copy data for pruned data\n",
    "    #original_data_size = np.sum(original_data.memory_usage(deep=True)) #how big is the data\n",
    "\n",
    "    #Prune Columns\n",
    "    pruned_data = pruned_data.drop(pruned_data.columns[ix], axis=1) #prune columns\n",
    "    \n",
    "    return pruned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prune the game logs\n",
    "def prune_gl_data(original_data, verbose=False):\n",
    "    #columns to drop. Doing by index again because it'd be a huge hassle to\n",
    "    #type hundreds of column names. I will convert the indices to col namse\n",
    "    #and add a verbose flag to see what I've chosen to drop. Again,\n",
    "    #i'm basically just reading the data description on retrosheet and doing\n",
    "    #this by hand. In the future, it would be easier to only pull the data we \n",
    "    #need as opposed to pulling all and then pruning\n",
    "    #gonna use lists to get ranges so i don't have to type tons of nums\n",
    "    ix = [13, 14, 15, 4, 5, 7, 8]\n",
    "    l1 = list(np.arange(78, 101, 1))\n",
    "    l2 = list(np.arange(106, 161, 1))\n",
    "    ix  = ix + l1 + l2 #concat these lists\n",
    "    \n",
    "    #get col names\n",
    "    ixn = original_data.columns[ix]\n",
    "    \n",
    "    #verbose option\n",
    "    if(verbose):\n",
    "        print(\"Dropping Columns:\\n\")\n",
    "        for col in ixn:\n",
    "            print(col)\n",
    "    \n",
    "    pruned_data = original_data #copy data for pruned data\n",
    "    \n",
    "    #Prune Columns\n",
    "    pruned_data = pruned_data.drop(pruned_data.columns[ix], axis=1) #prune columns\n",
    "    \n",
    "    return pruned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this method takes the event data and, for each event, adds a column for the\n",
    "#team under which the play is filed, the date of the play (for weather)\n",
    "#and the game of the day\n",
    "def add_ev_metadata(event_data):\n",
    "    #Adding in a bit more data on the team, date, game of day, and also field\n",
    "    event_data['team'] = event_data['game id'].astype(str).str[:3]#storing the team\n",
    "    event_data['date'] = event_data['game id'].astype(str).str[3:-1]#date\n",
    "    event_data['game_of_day'] = event_data['game id'].astype(str).str[-1:]#which game of the day it is\n",
    "    return event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in park data, city data, and station data respectively.\n",
    "#adds the coordinates and respective weather station to each park entry\n",
    "def add_pk_metadata(pdata, cdata, sdata):\n",
    "    #adding coordinates\n",
    "    pdata = pdata.merge(cdata, left_on='location', right_on='location', how='left')\n",
    "    \n",
    "    #now find the nearest weather station\n",
    "    pdata['station'] = 'na'#create an empty station col\n",
    "    combined = pd.DataFrame() #empty combined frame\n",
    "    sdata['parkid'] = 'zz' #so i can add them to the frame and it's easier to separate\n",
    "    combined['parkid'] = pdata['parkid'].append(sdata['parkid'], ignore_index=True) #add this to frame\n",
    "    lats = cdata['lat'].append(sdata['lat'], ignore_index=True) #add latitudes\n",
    "    combined['lat'] = lats\n",
    "    lngs = cdata['lng'].append(sdata['lng'], ignore_index=True) #and lngs\n",
    "    combined['lng'] = lngs\n",
    "    \n",
    "    #gonna use a nearest neighbors algorithm that uses lat and long\n",
    "    #to find the single nearest neighbor to each point.\n",
    "    #will use the combined cities and stations data so that it matches\n",
    "    #a city with the nearest station. I'm using cities because there are multiple\n",
    "    #parks per city, so they would just match with each other instead of a station.\n",
    "    X = [combined['lat'],combined['lng']]\n",
    "    nbrs = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "    indices = nbrs.kneighbors(X, n_neighbors=1)\n",
    "\n",
    "    print(combined.shape)\n",
    "    return pdata, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67530, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.],\n",
       "        [ 0.]]), array([[0],\n",
       "        [1]], dtype=int64))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkdata2, indices = add_pk_metadata(pkdata, ctdata, stdata)\n",
    "#pkdata2.shape\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parkid</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALB01</td>\n",
       "      <td>47.1443</td>\n",
       "      <td>-122.1408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>zz</td>\n",
       "      <td>47.1789</td>\n",
       "      <td>-122.1698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    parkid      lat       lng\n",
       "0    ALB01  47.1443 -122.1408\n",
       "631     zz  47.1789 -122.1698"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now find the nearest weather station\n",
    "pkdata['station'] = 'na'#create an empty station col\n",
    "combined = pd.DataFrame() #empty combined frame\n",
    "stdata['parkid'] = 'zz' #so i can add them to the frame and it's easier to separate\n",
    "combined['parkid'] = pkdata['parkid'].append(stdata['parkid'], ignore_index=True) #add this to frame\n",
    "lats = ctdata['lat'].append(stdata['lat'], ignore_index=True) #add latitudes\n",
    "combined['lat'] = lats\n",
    "lngs = ctdata['lng'].append(stdata['lng'], ignore_index=True) #and lngs\n",
    "combined['lng'] = lngs\n",
    "\n",
    "\n",
    "\n",
    "#gonna use a nearest neighbors algorithm that uses lat and long\n",
    "#to find the single nearest neighbor to each point.\n",
    "#will use the combined cities and stations data so that it matches\n",
    "#a city with the nearest station. I'm using cities because there are multiple\n",
    "#parks per city, so they would just match with each other instead of a station.\n",
    "X = np.column_stack((combined['lat'],combined['lng']))\n",
    "nbrs = NearestNeighbors(n_neighbors=2).fit(X)\n",
    "\n",
    "d, i  = nbrs.kneighbors(X)\n",
    "combined.iloc[i[0],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Categorical Data to Ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes any categorical data that can be represented as\n",
    "#ints and converts it. it also reduces all int columns\n",
    "#to the smallest possible integer form\n",
    "def reduce_ev_nums(original_data):\n",
    "    redata = original_data #reduced event data\n",
    "\n",
    "    #I noticed all the features I wanted to convert were all binary data for\n",
    "    #flags and which hand people use. So gonna iterate through the features\n",
    "    #and convert features with the word flag or hand\n",
    "    for column in redata.columns:#go through the cols\n",
    "        #if the feature name has hand or flag\n",
    "        if(('hand' in column.lower()) or ('flag' in column.lower())):\n",
    "            redata[column] = redata[column].astype('category')#set to cat\n",
    "\n",
    "    #convert them to ints            \n",
    "    cat_columns = redata.select_dtypes(['category']).columns #get cat cols\n",
    "    redata[cat_columns] = redata[cat_columns].apply(lambda x: x.cat.codes) #cast to int\n",
    "    \n",
    "    #downcast all to smallest acceptable int size\n",
    "    ints = redata.select_dtypes(include=['int']) #get any int cols\n",
    "    ints = ints.apply(pd.to_numeric, downcast='unsigned') #downcast them\n",
    "    \n",
    "    #downcast floats\n",
    "    floats = redata.select_dtypes(include=['float']) #get floats\n",
    "    floats = floats.apply(pd.to_numeric,downcast='float') #downcast them\n",
    "    \n",
    "    #now replace the numeric columns with their reduced ones\n",
    "    redata[ints.columns] = ints #replace with reduced ints\n",
    "    redata[floats.columns] = floats #replace with reduced floats\n",
    "    \n",
    "    #retyped_data_size = np.sum(redata.memory_usage(deep=True)) #store size\n",
    "    return redata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function will reduce the numbers in the game log data\n",
    "def reduce_gl_nums(original_data, verbose=False):\n",
    "    redata = original_data #copy data\n",
    "    \n",
    "    #manually looked through at which object cols could be cast to ints and \n",
    "    #reduced. Converting some like player name would cause us to lose info\n",
    "    #at the same time, we aren't really interested in specific pitchers so \n",
    "    #that may be a good idea to reduce them as well. future decisions.\n",
    "    ixn = ['DayOfWeek','DayNight'] #names for verbose\n",
    "    ix = [] #to store indices of names\n",
    "    for i in ixn: #iterate over col names\n",
    "        ix.append(original_data.columns.get_loc(i)) #get index from name\n",
    "    \n",
    "    #print names if verbose\n",
    "    if(verbose):\n",
    "        print(\"Removing Columns:\\n\")\n",
    "        for col in ixn:\n",
    "            print(col)\n",
    "            \n",
    "    #convert ints to categories\n",
    "    for col in ixn:\n",
    "        redata[col] = redata[col].astype('category')#cast to category\n",
    "    \n",
    "    #convert categories to ints\n",
    "    cat_cols = redata.select_dtypes(['category']).columns #get cat cols\n",
    "    redata[cat_cols] = redata[cat_cols].apply(lambda x: x.cat.codes) #to int\n",
    "    \n",
    "    #downcast ints\n",
    "    ints = redata.select_dtypes(include=['int']) #get ints\n",
    "    ints = ints.apply(pd.to_numeric,downcast='unsigned') #downcast them\n",
    "    \n",
    "    #downcast floats\n",
    "    floats = redata.select_dtypes(include=['float']) #get floats\n",
    "    floats = floats.apply(pd.to_numeric,downcast='float') #downcast them\n",
    "    \n",
    "    #now replace the numeric columns with their reduced ones\n",
    "    redata[ints.columns] = ints #replace with reduced ints\n",
    "    redata[floats.columns] = floats #replace with reduced floats\n",
    "    \n",
    "    return redata #return the reduced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function just combines the preprocessing steps so you don't have to\n",
    "# it takes a dataframe and a frame type eg: ev for event, gl for game log,\n",
    "# pk for park, wt for weather. It will then return the processed data and,\n",
    "# if you pass verbose as True, it'll return the dataframe size at each step\n",
    "# stored in a list\n",
    "def process_data(data, data_type, verbose=False):\n",
    "    sizes = [] #to store data sizes\n",
    "    reduced = pd.DataFrame() #to store reduced data\n",
    "    \n",
    "    #store first mem usage for verbose\n",
    "    sizes.append(np.sum(data.memory_usage(deep=True)))\n",
    "    \n",
    "    #process depending on data type\n",
    "    if(data_type=='ev'): #event data\n",
    "        reduced = prune_ev_data(data) #prune\n",
    "        sizes.append(np.sum(reduced.memory_usage(deep=True))) #verbose\n",
    "        reduced = reduce_ev_nums(reduced) #downcast numerics\n",
    "        sizes.append(np.sum(reduced.memory_usage(deep=True))) #verbose\n",
    "    elif(data_type=='gl'):\n",
    "        reduced = prune_gl_data(data) #prune\n",
    "        sizes.append(np.sum(reduced.memory_usage(deep=True))) #verbose\n",
    "        reduced = reduce_gl_nums(reduced) #downcast numerics\n",
    "        sizes.append(np.sum(reduced.memory_usage(deep=True))) #verbose\n",
    "    else:\n",
    "        print(\"Please enter a valid data type.\")\n",
    "        return\n",
    "    \n",
    "    #return depending on verbose\n",
    "    if(verbose): #return the sizes\n",
    "        return reduced, sizes #like so\n",
    "    else: #if they don't want verbose just return data\n",
    "        return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Frames and Processing Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "evdata = get_event_data() #get event data\n",
    "gldata = get_gl_data() #get game log data\n",
    "pkdata = get_park_data() #get park data\n",
    "ctdata = get_ct_data() #get city data\n",
    "stdata = get_st_data() #get the station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is an example of how to use the process data function\n",
    "reduced_evdata = process_data(evdata, 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data9 = EVdata.loc[data.inning==9]\n",
    "data92 = data9[abs(data9['vis score']-data9['home score'])<2]#9th inning with two outs\n",
    "x = data9['outs']\n",
    "y = data9['RBI on play']\n",
    "correlation = np.corrcoef(x, y)\n",
    "EVdata[\"outOrHit\"] = EVdata.apply(lambda row: 1 if \n",
    "                                (row[\"event type\"]==2 or \n",
    "                                row[\"event type\"]==3) else\n",
    "                                2 if \n",
    "                                row[\"event type\"]==20 else\n",
    "                                3 if\n",
    "                                row[\"event type\"]==21 else\n",
    "                                4 if\n",
    "                                row[\"event type\"]==22 else\n",
    "                                5 if\n",
    "                                row[\"event type\"]==23\n",
    "                                else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.hist(EVdata.loc[data.outs==2].outOrHit.dropna(), facecolor='green')\n",
    "ax.set_xlabel(\"Event Value\\nFigure 2\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xticks(np.arange(6))\n",
    "ax.set_title(\"Frequency of Event Values with 2 Outs\")\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure and axis\n",
    "fig, ax = plt.subplots(figsize=(4,12), nrows=3, ncols=1)\n",
    "for i in data.loc[data.inning==9].outs.unique():\n",
    "    EVdata.loc[(data.inning==9) & (data.outs==i)].outOrHit.hist(ax=ax[i])\n",
    "    ax[i].set_xlabel(\"Event Value\\nFigure 1\")\n",
    "    ax[i].set_ylabel(\"Frequency\")\n",
    "    ax[i].set_xticks(np.arange(6))\n",
    "    ax[i].set_title(\"Frequency of Event Values with {0} Outs\".format(i))\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "plt.suptitle(\"Frequency of Event Values with 0, 1, and 2 Outs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying To Make Some Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
