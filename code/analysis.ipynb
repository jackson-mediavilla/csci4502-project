{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods To Import Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function will take as an argument a file containing\n",
    "the field index and description for every field in the data frames.\n",
    "I will use this to create a template dataframe and to name the columns\n",
    "because currently it is just using the first value as the column name\n",
    "and it makes literally no sense at all.\n",
    "'''\n",
    "def shape_dataframe():\n",
    "    #this file contains all the field descriptions\n",
    "    field_descriptor_file_path = \"csvFieldDescriptions.txt\"\n",
    "    \n",
    "    #open the file and read it, adding each description to a list\n",
    "    fields = open(field_descriptor_file_path, 'r')\n",
    "    fieldInfo = []\n",
    "    for field in fields:\n",
    "        fieldInfo.append(field[2:].strip())\n",
    "    return fieldInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as an argument the number of subframes to include\n",
    "where each 'subframe' is a pandas data frame containing play by play\n",
    "data for a team for a year. It will then return those subframes\n",
    "concatenated together into one main data frame. Subframes is preset to 50\n",
    "'''\n",
    "def create_dataframe(subframes=50):\n",
    "    csv_paths = os.listdir('./csvFiles')#Get the paths of all of the CSV files\n",
    "    column_info = shape_dataframe()#get the information on each field\n",
    "    data = pd.DataFrame(columns=column_info)#create an empty data frame\n",
    "    individual_data = []#create a list to hold the smaller frames\n",
    "    del individual_data[:]#clear the list just in case I've already been using it\n",
    "\n",
    "    #make all the dataframes\n",
    "    df = pd.DataFrame(columns=column_info) #define a temp frame\n",
    "    for index, path in enumerate(csv_paths):#iterate over the list of paths\n",
    "        #names=column_info is what names the columns\n",
    "        df = pd.read_csv(str('./csvFiles/'+path), names=column_info)#read a file into a csv\n",
    "        individual_data.append(df)#add it to the list\n",
    "        \n",
    "        #this line here is what limits how much data you pull.\n",
    "        #if you eneter -1 for subframes it'll skip this check and\n",
    "        #generate all of the data\n",
    "        if(subframes != -1):#if the passed parameter is -1, generate ALL data\n",
    "            if(index==subframes):#stop when desired subframe # is reached\n",
    "                break\n",
    "    \n",
    "    data = pd.concat(individual_data) #combining sub frames\n",
    "    del individual_data[:] #dont wanna waste space\n",
    "    return data#, individual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method To Import Game Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in all the game logs and appending the dataframe accordingly\n",
    "def get_gl_data():\n",
    "    GLData = pd.DataFrame()\n",
    "    gls = [] #to hold smaller frames\n",
    "    del gls[:] #clear it to be sure\n",
    "    glogpath = '../data/GameLogs/' #path to the game logs\n",
    "    headPath = glogpath+'game_log_header.csv' #get the column info\n",
    "    header = pd.read_csv(headPath) #import the column info\n",
    "    colInfo = header.columns #store it for later use\n",
    "    start_year = 1950 #define starting year. gonna use this in path\n",
    "    end_year = 17 #also for path\n",
    "    for logFolder in os.listdir(glogpath):\n",
    "        try:#catching non int cases\n",
    "            y1 = int(logFolder[2:4]) ##first two of start yyyy\n",
    "            y2 = int(logFolder[2:6]) ##full start year\n",
    "            y3 = int(str(y1)+logFolder[-2:]) #full end year\n",
    "        except:\n",
    "            continue\n",
    "        #now open the folder if the start year is between the dates in the name\n",
    "        if(start_year<y3):\n",
    "            newPath = glogpath+logFolder\n",
    "            files = os.listdir(newPath)#get all the logs in the folder\n",
    "            for file in files:#now check each file to make sure it's the righ year\n",
    "                year = int(file[2:6])#get year of file\n",
    "                if(year>=start_year):#if it's within the range we want\n",
    "                    full_path = glogpath+logFolder+'/'+file #make full path\n",
    "                    gls.append(pd.read_csv(full_path, names=colInfo))#append the frame\n",
    "    GLData = pd.concat(gls) #combine the subframes into this one\n",
    "    del gls[:] #not wasting space\n",
    "    return GLData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should probably turn these into functions so I can also preprocess the main dataset. or any of the data that I want to pass to it. Shouldn't be to hard to parameterize. I guess it's worth noting that all of these functions so far have been written with the event data in mind. They almost certainly will not work on the gamelog data. I'm gonna rename the functions to indicate what data they should be used on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to see what fraction of the original size the pruned data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this will just return how much memory pre, post and the percentage\n",
    "def get_reduction(pre, post):\n",
    "    a = np.sum(pre.memory_usage())\n",
    "    b = np.sum(post.memory_usage())\n",
    "    return a, b, b/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to drop a lot of columns\n",
    "def prune_event_data(original_data, verbose=False):\n",
    "    #indexes for columns to drop. Honestly i just looked at the field descriptions\n",
    "    #and dropped mostly things like who was playing each position, where the ball \n",
    "    #was hit, the names of people who contributed to the play, etc\n",
    "    ix = [12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 32, 46, 47,\n",
    "          49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
    "          67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86,\n",
    "          87, 88, 89, 90, 91, 92, 93, 94, 95, 96\n",
    "         ]\n",
    "    #gonna turn those indexes into names just to ensure i drop the right\n",
    "    #columns\n",
    "    ixn = original_data.columns[ix]\n",
    "    \n",
    "    #verbose option\n",
    "    if(verbose):\n",
    "        print(\"Dropping Columns:\\n\")\n",
    "        for col in ixn:\n",
    "            print(col)\n",
    "    \n",
    "    pruned_data = original_data #copy data for pruned data\n",
    "    #original_data_size = np.sum(original_data.memory_usage(deep=True)) #how big is the data\n",
    "\n",
    "    #Prune Columns\n",
    "    pruned_data = pruned_data.drop(pruned_data.columns[ixn], axis=1) #prune columns\n",
    "    \n",
    "    return pruned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prune the game logs\n",
    "def prune_gl_data(original_data, verbose=False):\n",
    "    #columns to drop. Doing by index again because it'd be a huge hassle to\n",
    "    #type hundreds of column names. I will convert the indices to col namse\n",
    "    #and add a verbose flag to see what I've chosen to drop. Again,\n",
    "    #i'm basically just reading the data description on retrosheet and doing\n",
    "    #this by hand. In the future, it would be easier to only pull the data we \n",
    "    #need as opposed to pulling all and then pruning\n",
    "    #gonna use lists to get ranges so i don't have to type tons of nums\n",
    "    ix = [14, 15, 16]\n",
    "    l1 = list(np.arange(78, 101, 1))\n",
    "    l2 = list(np.arange(106, 161, 1))\n",
    "    ix  = ix + l1 + l2 #concat these lists\n",
    "    \n",
    "    #get col names\n",
    "    ixn = original_data.columns[ix]\n",
    "    \n",
    "    #verbose option\n",
    "    if(verbose):\n",
    "        print(\"Dropping Columns:\\n\")\n",
    "        for col in ixn:\n",
    "            print(col)\n",
    "    \n",
    "    pruned_data = original_data #copy data for pruned data\n",
    "    \n",
    "    #Prune Columns\n",
    "    pruned_data = pruned_data.drop(pruned_data.columns[ixn], axis=1) #prune columns\n",
    "    \n",
    "    return pruned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this method takes the event data and, for each event, adds a column for the\n",
    "#team under which the play is filed, the date of the play (for weather)\n",
    "#and the game of the day\n",
    "def add_event_metadata(event_data):\n",
    "    #Adding in a bit more data on the team, date, game of day, and also field\n",
    "    event_data['team'] = event_data['game id'].astype(str).str[:3]#storing the team\n",
    "    event_data['date'] = event_data['game id'].astype(str).str[3:-1]#date\n",
    "    event_data['game_of_day'] = event_data['game id'].astype(str).str[-1:]#which game of the day it is\n",
    "    return event_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Categorical Data to Ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes any categorical data that can be represented as\n",
    "#ints and converts it. it also reduces all int columns\n",
    "#to the smallest possible integer form\n",
    "def reduce_event_ints(original_data):\n",
    "    redata = original_data #reduced event data\n",
    "\n",
    "    #I noticed all the features I wanted to convert were all binary data for\n",
    "    #flags and which hand people use. So gonna iterate through the features\n",
    "    #and convert features with the word flag or hand\n",
    "    for column in redata.columns:#go through the cols\n",
    "        #if the feature name has hand or flag\n",
    "        if(('hand' in column.lower()) or ('flag' in column.lower())):\n",
    "            redata[column] = redata[column].astype('category')#set to cat\n",
    "\n",
    "    #convert them to ints            \n",
    "    cat_columns = redata.select_dtypes(['category']).columns #get cat cols\n",
    "    redata[cat_columns] = redata[cat_columns].apply(lambda x: x.cat.codes) #cast to int\n",
    "    ints = redata.select_dtypes(include=['int8', 'int64', 'int']) #get any int cols\n",
    "    ints = ints.columns #get feature names of ints\n",
    "    #downcast all to smallest acceptable int size\n",
    "    redata[ints] = redata[ints].apply(lambda x: pd.to_numeric(x, downcast='unsigned'))\n",
    "    #retyped_data_size = np.sum(redata.memory_usage(deep=True)) #store size\n",
    "    return redata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function will reduce the ints in the game log data\n",
    "def reduce_gl_ints(original_data):\n",
    "    redata = original_data #copy data\n",
    "    \n",
    "    #manually looked through at which object cols could be cast to ints and \n",
    "    #reduced. Converting some like player name would cause us to lose info\n",
    "    #at the same time, we aren't really interested in specific pitchers so \n",
    "    #that may be a good idea to reduce them as well. future decisions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Frames and Processing Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EVdata = create_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLData = get_gl_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(original_data_size, pruned_data_size, retyped_data_size)\n",
    "#print(original_data_size/original_data_size, pruned_data_size/original_data_size, retyped_data_size/original_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1950', '1951', '1952', '1953'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVdata = add_metadata(EVdata)\n",
    "EVdata.date.astype(str).str[:4].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data9 = EVdata.loc[data.inning==9]\n",
    "data92 = data9[abs(data9['vis score']-data9['home score'])<2]#9th inning with two outs\n",
    "x = data9['outs']\n",
    "y = data9['RBI on play']\n",
    "correlation = np.corrcoef(x, y)\n",
    "EVdata[\"outOrHit\"] = EVdata.apply(lambda row: 1 if \n",
    "                                (row[\"event type\"]==2 or \n",
    "                                row[\"event type\"]==3) else\n",
    "                                2 if \n",
    "                                row[\"event type\"]==20 else\n",
    "                                3 if\n",
    "                                row[\"event type\"]==21 else\n",
    "                                4 if\n",
    "                                row[\"event type\"]==22 else\n",
    "                                5 if\n",
    "                                row[\"event type\"]==23\n",
    "                                else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.hist(EVdata.loc[data.outs==2].outOrHit.dropna(), facecolor='green')\n",
    "ax.set_xlabel(\"Event Value\\nFigure 2\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xticks(np.arange(6))\n",
    "ax.set_title(\"Frequency of Event Values with 2 Outs\")\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure and axis\n",
    "fig, ax = plt.subplots(figsize=(4,12), nrows=3, ncols=1)\n",
    "for i in data.loc[data.inning==9].outs.unique():\n",
    "    EVdata.loc[(data.inning==9) & (data.outs==i)].outOrHit.hist(ax=ax[i])\n",
    "    ax[i].set_xlabel(\"Event Value\\nFigure 1\")\n",
    "    ax[i].set_ylabel(\"Frequency\")\n",
    "    ax[i].set_xticks(np.arange(6))\n",
    "    ax[i].set_title(\"Frequency of Event Values with {0} Outs\".format(i))\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "plt.suptitle(\"Frequency of Event Values with 0, 1, and 2 Outs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying To Make Some Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
